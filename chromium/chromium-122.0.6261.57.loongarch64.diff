diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/base/allocator/partition_allocator/partition_alloc.gni b/base/allocator/partition_allocator/partition_alloc.gni
--- a/base/allocator/partition_allocator/partition_alloc.gni	2024-02-21 08:20:30.437336000 +0800
+++ b/base/allocator/partition_allocator/partition_alloc.gni	2024-02-21 20:50:05.401247722 +0800
@@ -234,7 +234,7 @@ use_starscan = build_with_chromium && ha
 pcscan_stack_supported =
     use_starscan &&
     (current_cpu == "x64" || current_cpu == "x86" || current_cpu == "arm" ||
-     current_cpu == "arm64" || current_cpu == "riscv64")
+     current_cpu == "arm64" || current_cpu == "riscv64" || current_cpu == "loong64")
 
 # We want to provide assertions that guard against inconsistent build
 # args, but there is no point in having them fire if we're not building
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/base/system/sys_info.cc b/base/system/sys_info.cc
--- a/base/system/sys_info.cc	2024-02-21 08:20:30.677353000 +0800
+++ b/base/system/sys_info.cc	2024-02-21 20:50:05.404247769 +0800
@@ -257,6 +257,8 @@ std::string SysInfo::ProcessCPUArchitect
   return "ARM_64";
 #elif defined(ARCH_CPU_RISCV64)
   return "RISCV_64";
+#elif defined(ARCH_CPU_LOONGARCH64)
+  return "LOONGARCH_64";
 #else
   return std::string();
 #endif
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/build/config/rust.gni b/build/config/rust.gni
--- a/build/config/rust.gni	2024-02-21 08:20:30.901368900 +0800
+++ b/build/config/rust.gni	2024-02-21 20:50:05.407247815 +0800
@@ -214,6 +214,8 @@ if (is_linux || is_chromeos) {
     } else {
       rust_abi_target = "arm-unknown-linux-gnueabi" + float_suffix
     }
+  } else if (current_cpu == "loong64") {
+    rust_abi_target = "loongarch64-unknown-linux-gnu"
   } else {
     # Best guess for other future platforms.
     rust_abi_target = current_cpu + "-unknown-linux-gnu"
@@ -302,6 +304,8 @@ if (current_cpu == "x86") {
   rust_target_arch = "powerpc64"
 } else if (current_cpu == "riscv64") {
   rust_target_arch = "riscv64"
+} else if (current_cpu == "loong64") {
+  rust_target_arch = "loongarch64"
 }
 
 assert(!toolchain_has_rust || rust_target_arch != "")
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/content/common/user_agent.cc b/content/common/user_agent.cc
--- a/content/common/user_agent.cc	2024-02-21 08:20:45.286389400 +0800
+++ b/content/common/user_agent.cc	2024-02-21 20:50:08.607297587 +0800
@@ -174,6 +174,8 @@ std::string GetCpuArchitecture() {
               cpu_info.substr(2, 2) == "86") ||
              base::StartsWith(cpu_info, "x86")) {
     return "x86";
+  } else if (base::StartsWith(cpu_info, "loong")) {
+    return "loongarch";
   }
 #elif BUILDFLAG(IS_FUCHSIA)
   std::string cpu_arch = base::SysInfo::ProcessCPUArchitecture();
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/skia/BUILD.gn b/skia/BUILD.gn
--- a/skia/BUILD.gn	2024-02-21 08:20:51.854855300 +0800
+++ b/skia/BUILD.gn	2024-02-21 20:50:10.311324091 +0800
@@ -747,6 +747,8 @@ skia_source_set("skia_opts") {
     # Conditional and empty body needed to avoid assert() below.
   } else if (current_cpu == "riscv64") {
     # Conditional and empty body needed to avoid assert() below.
+  } else if (current_cpu == "loong64") {
+    # Conditional and empty body needed to avoid assert() below.
   } else {
     assert(false, "Unknown cpu target")
   }
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/third_party/blink/renderer/platform/graphics/cpu/loongarch64/webgl_image_conversion_lsx.h b/third_party/blink/renderer/platform/graphics/cpu/loongarch64/webgl_image_conversion_lsx.h
--- a/third_party/blink/renderer/platform/graphics/cpu/loongarch64/webgl_image_conversion_lsx.h	2024-02-21 08:20:54.551046600 +0800
+++ b/third_party/blink/renderer/platform/graphics/cpu/loongarch64/webgl_image_conversion_lsx.h	2024-02-21 20:50:11.989350190 +0800
@@ -91,19 +91,19 @@ ALWAYS_INLINE void PackOneRowOfRGBA8Litt
   v4f32 mask_falpha = __lsx_vffint_s_w(mask_lalpha);
   v16u8 ra_index = {0,19, 4,23, 8,27, 12,31};
   for (unsigned i = 0; i < pixels_per_row_trunc; i += 4) {
-    v16u8 bgra = *((__m128i*)(source));
+    __m128i bgra = *((__m128i*)(source));
     //if A !=0, A=0; else A=0xFF
-    v4f32 alpha_factor = __lsx_vseq_b(bgra, mask_zero);
+    __m128i alpha_factor = __lsx_vseq_b(bgra, mask_zero);
     //if A!=0, A=A; else A=0xFF
     alpha_factor = __lsx_vor_v(bgra, alpha_factor);
     alpha_factor = __lsx_vsrli_w(alpha_factor, 24);
-    alpha_factor = __lsx_vffint_s_w(alpha_factor);
-    alpha_factor = __lsx_vfdiv_s(mask_falpha, alpha_factor);
+    alpha_factor = (__m128i) __lsx_vffint_s_w(alpha_factor);
+    alpha_factor = (__m128i) __lsx_vfdiv_s((__m128) mask_falpha, (__m128) alpha_factor);
 
-    v16u8 component_r = __lsx_vand_v(bgra, mask_lalpha);
-    component_r = __lsx_vffint_s_w(component_r);
-    component_r = __lsx_vfmul_s(component_r, alpha_factor);
-    component_r = __lsx_vftintrz_w_s(component_r);
+    __m128i component_r = __lsx_vand_v(bgra, mask_lalpha);
+    component_r = (__m128i) __lsx_vffint_s_w(component_r);
+    component_r = (__m128i) __lsx_vfmul_s((__m128) component_r, (__m128) alpha_factor);
+    component_r = __lsx_vftintrz_w_s((__m128) component_r);
 
     v2u64 ra = __lsx_vshuf_b(bgra, component_r, ra_index);
     __lsx_vstelm_d(ra, destination, 0, 0);
@@ -138,19 +138,19 @@ ALWAYS_INLINE void PackOneRowOfRGBA8Litt
   v4u32 mask_lalpha = __lsx_vreplgr2vr_w(0x0ff);
   v4f32 mask_falpha = __lsx_vffint_s_w(mask_lalpha);
   for (unsigned i = 0; i < pixels_per_row_trunc; i += 4) {
-    v16u8 bgra = *((__m128i*)(source));
+    __m128i bgra = *((__m128i*)(source));
     //if A !=0, A=0; else A=0xFF
-    v4f32 alpha_factor = __lsx_vseq_b(bgra, mask_zero);
+    __m128i alpha_factor = __lsx_vseq_b(bgra, mask_zero);
     //if A!=0, A=A; else A=0xFF
     alpha_factor = __lsx_vor_v(bgra, alpha_factor);
     alpha_factor = __lsx_vsrli_w(alpha_factor, 24);
-    alpha_factor = __lsx_vffint_s_w(alpha_factor);
-    alpha_factor = __lsx_vfdiv_s(mask_falpha, alpha_factor);
+    alpha_factor = (__m128i) __lsx_vffint_s_w(alpha_factor);
+    alpha_factor = (__m128i) __lsx_vfdiv_s((__m128) mask_falpha, (__m128) alpha_factor);
 
-    v16u8 component_r = __lsx_vand_v(bgra, mask_lalpha);
-    component_r = __lsx_vffint_s_w(component_r);
-    component_r = __lsx_vfmul_s(component_r, alpha_factor);
-    component_r = __lsx_vftintrz_w_s(component_r);
+    __m128i component_r = __lsx_vand_v(bgra, mask_lalpha);
+    component_r = (__m128i) __lsx_vffint_s_w(component_r);
+    component_r = (__m128i) __lsx_vfmul_s((__m128) component_r, (__m128) alpha_factor);
+    component_r = __lsx_vftintrz_w_s((__m128) component_r);
 
     component_r = __lsx_vpickev_b(component_r, component_r);
     component_r = __lsx_vpickev_b(component_r, component_r);
@@ -173,41 +173,41 @@ ALWAYS_INLINE void PackOneRowOfRGBA8Litt
   v4f32 mask_falpha = __lsx_vffint_s_w(mask_lalpha);
   v16u8 rgba_index = {0,1,2,19, 4,5,6,23, 8,9,10,27, 12,13,14,31};
   for (unsigned i = 0; i < pixels_per_row_trunc; i += 4) {
-    v16u8 bgra = *((__m128i*)(source));
+    __m128i bgra = *((__m128i*)(source));
     //if A !=0, A=0; else A=0xFF
-    v4f32 alpha_factor = __lsx_vseq_b(bgra, mask_zero);
+    __m128i alpha_factor = __lsx_vseq_b(bgra, mask_zero);
     //if A!=0, A=A; else A=0xFF
     alpha_factor = __lsx_vor_v(bgra, alpha_factor);
     alpha_factor = __lsx_vsrli_w(alpha_factor, 24);
-    alpha_factor = __lsx_vffint_s_w(alpha_factor);
-    alpha_factor = __lsx_vfdiv_s(mask_falpha, alpha_factor);
+    alpha_factor = (__m128i) __lsx_vffint_s_w(alpha_factor);
+    alpha_factor = (__m128i) __lsx_vfdiv_s((__m128) mask_falpha, (__m128) alpha_factor);
 
     v16u8 bgra_01 = __lsx_vilvl_b(mask_zero, bgra);
     v16u8 bgra_23 = __lsx_vilvh_b(mask_zero, bgra);
-    v16u8 bgra_0 = __lsx_vilvl_b(mask_zero, bgra_01);
-    v16u8 bgra_1 = __lsx_vilvh_b(mask_zero, bgra_01);
-    v16u8 bgra_2 = __lsx_vilvl_b(mask_zero, bgra_23);
-    v16u8 bgra_3 = __lsx_vilvh_b(mask_zero, bgra_23);
-
-    bgra_0 = __lsx_vffint_s_w(bgra_0);
-    bgra_1 = __lsx_vffint_s_w(bgra_1);
-    bgra_2 = __lsx_vffint_s_w(bgra_2);
-    bgra_3 = __lsx_vffint_s_w(bgra_3);
-
-    v4f32 alpha_factor_0 = __lsx_vreplvei_w(alpha_factor, 0);
-    v4f32 alpha_factor_1 = __lsx_vreplvei_w(alpha_factor, 1);
-    v4f32 alpha_factor_2 = __lsx_vreplvei_w(alpha_factor, 2);
-    v4f32 alpha_factor_3 = __lsx_vreplvei_w(alpha_factor, 3);
-
-    bgra_0 = __lsx_vfmul_s(alpha_factor_0, bgra_0);
-    bgra_1 = __lsx_vfmul_s(alpha_factor_1, bgra_1);
-    bgra_2 = __lsx_vfmul_s(alpha_factor_2, bgra_2);
-    bgra_3 = __lsx_vfmul_s(alpha_factor_3, bgra_3);
-
-    bgra_0 = __lsx_vftintrz_w_s(bgra_0);
-    bgra_1 = __lsx_vftintrz_w_s(bgra_1);
-    bgra_2 = __lsx_vftintrz_w_s(bgra_2);
-    bgra_3 = __lsx_vftintrz_w_s(bgra_3);
+    __m128i bgra_0 = __lsx_vilvl_b(mask_zero, bgra_01);
+    __m128i bgra_1 = __lsx_vilvh_b(mask_zero, bgra_01);
+    __m128i bgra_2 = __lsx_vilvl_b(mask_zero, bgra_23);
+    __m128i bgra_3 = __lsx_vilvh_b(mask_zero, bgra_23);
+
+    bgra_0 = (__m128i) __lsx_vffint_s_w(bgra_0);
+    bgra_1 = (__m128i) __lsx_vffint_s_w(bgra_1);
+    bgra_2 = (__m128i) __lsx_vffint_s_w(bgra_2);
+    bgra_3 = (__m128i) __lsx_vffint_s_w(bgra_3);
+
+    __m128i alpha_factor_0 = __lsx_vreplvei_w(alpha_factor, 0);
+    __m128i alpha_factor_1 = __lsx_vreplvei_w(alpha_factor, 1);
+    __m128i alpha_factor_2 = __lsx_vreplvei_w(alpha_factor, 2);
+    __m128i alpha_factor_3 = __lsx_vreplvei_w(alpha_factor, 3);
+
+    bgra_0 = (__m128i) __lsx_vfmul_s((__m128) alpha_factor_0, (__m128) bgra_0);
+    bgra_1 = (__m128i) __lsx_vfmul_s((__m128) alpha_factor_1, (__m128) bgra_1);
+    bgra_2 = (__m128i) __lsx_vfmul_s((__m128) alpha_factor_2, (__m128) bgra_2);
+    bgra_3 = (__m128i) __lsx_vfmul_s((__m128) alpha_factor_3, (__m128) bgra_3);
+
+    bgra_0 = __lsx_vftintrz_w_s((__m128) bgra_0);
+    bgra_1 = __lsx_vftintrz_w_s((__m128) bgra_1);
+    bgra_2 = __lsx_vftintrz_w_s((__m128) bgra_2);
+    bgra_3 = __lsx_vftintrz_w_s((__m128) bgra_3);
 
     bgra_01 = __lsx_vpickev_b(bgra_1, bgra_0);
     bgra_23 = __lsx_vpickev_b(bgra_3, bgra_2);
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/third_party/boringssl/src/include/openssl/target.h b/third_party/boringssl/src/include/openssl/target.h
--- a/third_party/boringssl/src/include/openssl/target.h	2024-02-21 08:21:23.133074300 +0800
+++ b/third_party/boringssl/src/include/openssl/target.h	2024-02-21 20:50:11.991350221 +0800
@@ -54,6 +54,8 @@
 #define OPENSSL_32_BIT
 #elif defined(__myriad2__)
 #define OPENSSL_32_BIT
+#elif defined(__loongarch__) && __SIZEOF_POINTER__ == 8
+#define OPENSSL_64_BIT
 #else
 // The list above enumerates the platforms that BoringSSL supports. For these
 // platforms we keep a reasonable bar of not breaking them: automated test
diff '--color=auto' -p -X ../chromium-loongarch64/chromium/exclude -N -u -r a/third_party/skia/modules/skcms/src/skcms_internals.h b/third_party/skia/modules/skcms/src/skcms_internals.h
--- a/third_party/skia/modules/skcms/src/skcms_internals.h	2024-02-21 08:21:32.705753300 +0800
+++ b/third_party/skia/modules/skcms/src/skcms_internals.h	2024-02-21 20:50:12.022350703 +0800
@@ -46,6 +46,7 @@ extern "C" {
                                                  && !defined(__EMSCRIPTEN__) \
                                                  && !defined(__arm__) \
                                                  && !defined(__riscv) \
+                                                 && !defined(__loongarch__) \
                                                  && !defined(_WIN32) && !defined(__SYMBIAN32__)
             #define SKCMS_HAS_MUSTTAIL 1
         #endif
